# Building Retrieval Augmented Generation (RAG) From Scratch
 LLMs can reason well on general data but when it comes to reasoning on specific data, LLMs don't perform well. They can’t perform well when it comes to data that was generated after LLMs were trained. To develop an application capable of understanding private data or information beyond what the model already knows, we need to enhance the model’s knowledge with specific details. The process of bringing the appropriate information and inserting it into the model to get answers on particular data is known as Retrieval Augmented Generation (RAG).  High-level functions offered by libraries like Llama and Langchain have made the development of RAG very easy. However, it is important to understand the inner workings and essential mechanisms of each step involved in building RAG. In this article, we will implement each step from scratch without using Llama and Langchain. In the end, when we will get the most relevant data from the documents we will then give it to the large language model to get a context-specific response from the large language model.
